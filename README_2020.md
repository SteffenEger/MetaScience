### Organization

The seminar will be held by [Steffen Eger](https://steffeneger.github.io/).

### Meetings
There will be 2-3 block meetings. The opening meeting will be via zoom on 20.04.2021 from 17 to 18.

### Latest news

See the [TU Moodle](https://moodle.informatik.tu-darmstadt.de/course/view.php?id=1043) for recent updates.

### Language
The seminar will be held in English.

### Topics

This year, two topics are of relevance:

   1. Evaluation metrics and evaluation paradigms (for Natural Language Processing)
   2. Science of science related topics such as citation count prediction, analysis of scientific literature, and analysis of biases in science

### Seminar structure

This year, there are two tracks:

  * **Regular track**: one student prepares 1-3 papers on one topic. Requirements:
       - offline video presentation (~20min) on the papers; due around late June
       - write a small report on one other video presentation (one paragraph); due early July
       - online group discussion (~10-20min); due around mid July
       - term paper (4-6 pages); due around mid September
  * **Project track**: 2-4 students jointly work on a project, proposed by us
       - each student in the team reads 1-3 papers (relating to the project's topic), to understand the background
       - the team works on a solution to the problem (this typically involves coding)
       - The team writes a paper (8-10 pages) on their project; due mid-late July
       - We will offer 2-3 discussions with each group on the implementation and the write-up during the semester

We will offer up to 5 slots for the regular track and up to 5 slots for the project track. For the project track, students are allowed to self-select into teams. 

Topics for the project track can be found here (continuously updated until April, 20): [Project ideas](https://docs.google.com/document/d/15EBPnYrz20CEF1a72MzvC0rvgmNLl8iBM5rOPiBQ_p4/edit?usp=sharing)


### Literature

* A: Overview articles
   - 1 [Science of Science](https://www.barabasilab.com/publications/science-of-science)
   - 2 [The science of science: From the perspective of complex systems](https://www.sciencedirect.com/science/article/pii/S0370157317303289) 
 
* B: Citations and Altmetrics 
   - 1 [Grand challenges in altmetrics: heterogeneity, data quality and dependencies
](https://link.springer.com/article/10.1007/s11192-016-1910-9)
   - 2 [Citations, Citation Indicators, and Research Quality: An Overview of Basic Concepts and Theories](https://journals.sagepub.com/doi/full/10.1177/2158244019829575)
   - 3 [Citation Classification for Behavioral Analysis of a Scientific Field](https://arxiv.org/abs/1609.00435)
 
* C: Citation Count Prediction and Prediction of new trends  
   - 1 [Predicting citation counts based on deep neural network learning techniques](https://arxiv.org/abs/1809.04365)
   - 2 [Can Scientific Impact Be Predicted?](https://arxiv.org/pdf/1606.05905.pdf)
   - 3 [The nearly universal link between the age of past knowledge and tomorrow’s breakthroughs in science and technology: The hotspot](https://advances.sciencemag.org/content/3/4/e1601315)
   - 4 [Predicting scientific success based on coauthorship networks](https://link.springer.com/article/10.1140/epjds/s13688-014-0009-x)
   - 5 [Predicting the Rise and Fall of Scientific Topics from Trends in their Rhetorical Framing](https://nlp.stanford.edu/pubs/prabhakaran2016rhetoricalroles.pdf)
   - 6 [Predicting Research Trends with Semantic and Neural Networks with an application in Quantum Physics](https://arxiv.org/abs/1906.06843)
   - 7 [Measuring the Evolution of a Scientific Field through Citation Frames](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00028/43437/Measuring-the-Evolution-of-a-Scientific-Field)
   - 8 [Structural Scaffolds for Citation Intent Classification in Scientific Publications](https://arxiv.org/abs/1904.01608)
   
* D: NLP for Scientific Text
   - 1 [ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks](https://arxiv.org/abs/1909.01716)
   - 2 [SciBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676)
   - 3 [The Computer Science Ontology: A Large-Scale
Taxonomy of Research Areas](http://oro.open.ac.uk/55484/14/55484.pdf)
   - 4 [Keyphrase Extraction from Scholarly Articles as Sequence Labeling using Contextualized Embeddings](https://arxiv.org/pdf/1910.08840.pdf)
   - 5 [Does my rebuttal matter? Insights from a major nlp conference](https://www.aclweb.org/anthology/N19-1129.pdf)
   - 6 [The Diversity–Innovation Paradox in Science](https://www.pnas.org/content/pnas/117/17/9284.full.pdf)
   - 8 [Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction](https://arxiv.org/abs/1906.09317)

* E: Biases and unethical behavior 
   - 1 [On Good and Bad Intentions behind Anomalous Citation Patterns among Journals in Computer Sciences](https://arxiv.org/abs/1807.10804)
   - 2 [Towards the discovery of citation cartels in citation networks](https://ui.adsabs.harvard.edu/abs/2016FrP.....4...49F/abstract)
   - (Opinion Article) [What is Ethics in Research & Why is it Important](https://www.veronaschools.org/cms/lib02/NJ01001379/Centricity/Domain/588/What%20is%20Ethics%20in%20Research%20Why%20is%20it%20Important.pdf)
   - 3 [Meta-assessment of bias in science](https://www.ncbi.nlm.nih.gov/pubmed/?term=Meta-assessment+of+bias+in+science)
   - (Opinion Article) [Predatory journals recruit fake editor](https://www.nature.com/articles/543481a)
   - 4 [Self-citations as strategic response to the use of metrics for career decisions](https://www.sciencedirect.com/science/article/abs/pii/S004873331730210X)
   - 5 [Scientific misconduct: the dark side of science](https://link.springer.com/article/10.1007/s12210-015-0415-4)
   - 6 [Scientific Misconduct](https://www.annualreviews.org/doi/abs/10.1146/annurev-psych-122414-033437)
   - 7 [Over-Optimization of Academic Publishing Metrics: Observing Goodhart's Law in Action](https://academic.oup.com/gigascience/article/8/6/giz053/5506490)

* F: Evaluation and Evaluation Metrics
   - more related work, especially on evaluation metrics: see the googleDoc
   - 1 [Show Your Work: Improved Reporting of Experimental Results](https://arxiv.org/abs/1909.03004)
   - 2 [On the State of the Art of Evaluation in Neural Language Models](https://arxiv.org/abs/1707.05589)
   - 3 [Realistic Evaluation of Deep Semi-Supervised Learning Algorithms](https://papers.nips.cc/paper/7585-realistic-evaluation-of-deep-semi-supervised-learning-algorithms.pdf)
   - 4 [Pitfalls of Graph Neural Network Evaluation](https://arxiv.org/abs/1811.05868)
   - 5 [Probing Neural Network Comprehension of Natural Language Arguments](https://arxiv.org/abs/1907.07355)
   - 6 [Evaluating NLP Models via Contrast Sets](https://arxiv.org/abs/2004.02709)
