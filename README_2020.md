### Organization

The seminar will be held by [Steffen Eger](https://steffeneger.github.io/).

### Meetings
There will be 2-3 block meetings. The opening meeting will be via zoom on 20.04.2021 from 17 to 18.

### Latest news

See the [TU Moodle](https://moodle.informatik.tu-darmstadt.de/course/view.php?id=1043) for recent updates.

### Language
The seminar will be held in English.

### Topics

This year, two topics are of relevance:

   1. Evaluation metrics and evaluation paradigms (for Natural Language Processing)
   2. Science of science related topics such as citation count prediction, analysis of scientific literature, and analysis of biases in science

### Seminar structure

This year, there are two tracks:

  * **Regular track**: one student prepares 1-3 papers on one topic. Requirements:
       - offline video presentation (~20min) on the papers; due around late June
       - write a small report on one other video presentation (one paragraph); due early July
       - online group discussion (~10-20min); due around mid July
       - term paper (4-6 pages); due around mid September
  * **Project track**: 2-4 students jointly work on a project, proposed by us
       - each student in the team reads 1-3 papers (relating to the project's topic), to understand the background
       - the team works on a solution to the problem (this typically involves coding)
       - The team writes a paper (8-10 pages) on their project; due mid-late July
       - We will offer 2-3 discussions with each group on the implementation and the write-up during the semester

We will offer up to 5 slots for the regular track and up to 5 slots for the project track. For the project track, students are allowed to self-select into teams. 

Topics for the project track can be found here (continuously updated until April, 20): [Project ideas](https://docs.google.com/document/d/15EBPnYrz20CEF1a72MzvC0rvgmNLl8iBM5rOPiBQ_p4/edit?usp=sharing)


### Literature

* A: Overview articles
   - 1 [Science of Science](https://www.barabasilab.com/publications/science-of-science)
   - 2 [The science of science: From the perspective of complex systems](https://www.sciencedirect.com/science/article/pii/S0370157317303289) 
 
* B: Citations and Altmetrics 
   - 1 [Grand challenges in altmetrics: heterogeneity, data quality and dependencies
](https://link.springer.com/article/10.1007/s11192-016-1910-9)
   - 2 [Citations, Citation Indicators, and Research Quality: An Overview of Basic Concepts and Theories](https://journals.sagepub.com/doi/full/10.1177/2158244019829575)
   - 3 [Citation Classification for Behavioral Analysis of a Scientific Field](https://arxiv.org/abs/1609.00435)
   - 4 [Status drives how we cite: Evidence from thousands of authors](https://arxiv.org/ftp/arxiv/papers/2002/2002.10033.pdf)
   - 5 [Structural Scaffolds for Citation Intent Classification in Scientific Publications](https://arxiv.org/abs/1904.01608)

* C: Citation Count Prediction and Prediction of new trends  
   - 1 [Predicting citation counts based on deep neural network learning techniques](https://arxiv.org/abs/1809.04365)
   - 2 [Can Scientific Impact Be Predicted?](https://arxiv.org/pdf/1606.05905.pdf)
   - 3 [The nearly universal link between the age of past knowledge and tomorrow’s breakthroughs in science and technology: The hotspot](https://advances.sciencemag.org/content/3/4/e1601315)
   - 4 [Predicting scientific success based on coauthorship networks](https://link.springer.com/article/10.1140/epjds/s13688-014-0009-x)
   - 5 [Predicting the Rise and Fall of Scientific Topics from Trends in their Rhetorical Framing](https://nlp.stanford.edu/pubs/prabhakaran2016rhetoricalroles.pdf)
   - 6 [Predicting Research Trends with Semantic and Neural Networks with an application in Quantum Physics](https://arxiv.org/abs/1906.06843)
   - 7 [Measuring the Evolution of a Scientific Field through Citation Frames](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00028/43437/Measuring-the-Evolution-of-a-Scientific-Field)
   - 8 [The use of citation context to detect the evolution of research topics: a large-scale analysis](https://link.springer.com/article/10.1007/s11192-020-03858-y)
   - 9 [Predicting the citation counts of individual papers via a BP neural network](https://www.sciencedirect.com/science/article/abs/pii/S1751157719303979)
   - 10 [A Neural Citation Count Prediction Modelbased on Peer Review Text](https://www.aclweb.org/anthology/D19-1497.pdf)
   - 11 [ TRENDNERT: A Benchmark for Trend and Downtrend Detection in a Scientific Domain ](https://ojs.aaai.org/index.php/AAAI/article/view/6372)

* D: NLP for Scientific Text
   - 1 [ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks](https://arxiv.org/abs/1909.01716)
   - 2 [SciBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676)
   - 3 [The Computer Science Ontology: A Large-Scale
Taxonomy of Research Areas](http://oro.open.ac.uk/55484/14/55484.pdf)
   - 4 [Does my rebuttal matter? Insights from a major nlp conference](https://www.aclweb.org/anthology/N19-1129.pdf)
   - 6 [Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction](https://arxiv.org/abs/1906.09317)
   - 7 [What Can We Do to Improve Peer Review in NLP?](https://arxiv.org/pdf/2010.03863.pdf)
   - 8 [Can We Automate Scientific Reviewing?](https://arxiv.org/abs/2102.00176)

* E: Biases and unethical behavior 
   - 1 [On Good and Bad Intentions behind Anomalous Citation Patterns among Journals in Computer Sciences](https://arxiv.org/abs/1807.10804)
   - 2 [Towards the discovery of citation cartels in citation networks](https://ui.adsabs.harvard.edu/abs/2016FrP.....4...49F/abstract)
   - (Opinion Article) [What is Ethics in Research & Why is it Important](https://www.veronaschools.org/cms/lib02/NJ01001379/Centricity/Domain/588/What%20is%20Ethics%20in%20Research%20Why%20is%20it%20Important.pdf)
   - 3 [Meta-assessment of bias in science](https://www.ncbi.nlm.nih.gov/pubmed/?term=Meta-assessment+of+bias+in+science)
   - (Opinion Article) [Predatory journals recruit fake editor](https://www.nature.com/articles/543481a)
   - 4 [Self-citations as strategic response to the use of metrics for career decisions](https://www.sciencedirect.com/science/article/abs/pii/S004873331730210X)
   - 5 [Scientific misconduct: the dark side of science](https://link.springer.com/article/10.1007/s12210-015-0415-4)
   - 6 [Scientific Misconduct](https://www.annualreviews.org/doi/abs/10.1146/annurev-psych-122414-033437)
   - 7 [Over-Optimization of Academic Publishing Metrics: Observing Goodhart's Law in Action](https://academic.oup.com/gigascience/article/8/6/giz053/5506490)
   - 8 [ How much is too much? The difference between research influence and self-citation excess ](https://link.springer.com/article/10.1007/s11192-020-03417-5)
   - 9 [Misinformation in and about science](https://www.pnas.org/content/118/15/e1912444117.short)
   - 10 [Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills? ](https://febs.onlinelibrary.wiley.com/doi/full/10.1002/1873-3468.13747)

* F1: Evaluation Pitfalls
   - more related work, especially on evaluation metrics: see the googleDoc
   - 1 [Show Your Work: Improved Reporting of Experimental Results](https://arxiv.org/abs/1909.03004)
   - 2 [On the State of the Art of Evaluation in Neural Language Models](https://arxiv.org/abs/1707.05589)
   - 3 [Realistic Evaluation of Deep Semi-Supervised Learning Algorithms](https://papers.nips.cc/paper/7585-realistic-evaluation-of-deep-semi-supervised-learning-algorithms.pdf)
   - 4 [Pitfalls of Graph Neural Network Evaluation](https://arxiv.org/abs/1811.05868)
   - 5 [Probing Neural Network Comprehension of Natural Language Arguments](https://arxiv.org/abs/1907.07355)
   - 6 [Evaluating NLP Models via Contrast Sets](https://arxiv.org/abs/2004.02709)
   - 7 [A call for more rigor in unsupervised cross-lingual learning](https://arxiv.org/abs/2004.14958)
   - 8 [Towards Debiasing NLU Models from Unknown Biases](https://arxiv.org/abs/2009.12303)

* F2: Evaluation
   - 1 Assessing Reference-Free Peer Evaluation for Machine Translation
   - 2 Robustness Gym: Unifying the NLP Evaluation Landscape
   - 3 SentSim: Crosslingual Semantic Evaluation of Machine Translation
   - 4 Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics
   - 5 [Statistical Power and Translationese in Machine Translation Evaluation](https://www.aclweb.org/anthology/2020.emnlp-main.6/)
   - 6 [COMET: A Neural Framework for MT Evaluation](https://www.aclweb.org/anthology/2020.emnlp-main.213.pdf)
   - 7 [Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](https://arxiv.org/pdf/2004.14564.pdf)
   - 8 Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning
   - 9 Re-evaluating Evaluation in Text Summarization
   - 10 Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples
   - 11 [Evaluating Explanation Methods for Neural Machine Translation](https://arxiv.org/abs/2005.01672)
   - 12 Facet-Aware Evaluation for Extractive Summarization
   - 13 FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization
   - 14 GLUECoS: An Evaluation Benchmark for Code-Switched NLP
   - 15 Multi-Hypothesis Machine Translation Evaluation
   - 16 On The Evaluation of Machine Translation Systems Trained With Back-Translation
   - 17 On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation
   - 18 Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics
   - 19 [Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model](https://www.aclweb.org/anthology/2020.acl-main.327/)
   - 20 SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization
   - 21 CLIReval: Evaluating Machine Translation as a Cross-Lingual Information Retrieval Task
   - 22 BLEURT: Learning Robust Metrics for Text Generation
   - 23 bleu might be guilty but references are not innocent
   - 24 [Unsupervised quality estimation for neural machine translation](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00330/96475/Unsupervised-Quality-Estimation-for-Neural-Machine)


* H: Other
   - 1 [Success and luck in creative careers](https://epjds.epj.org/articles/epjdata/abs/2020/01/13688_2020_Article_227/13688_2020_Article_227.html) 
   - 2 [The coauthorship networks of the most productive European researchers](https://link.springer.com/article/10.1007/s11192-020-03746-5)
   - 3 [The Diversity–Innovation Paradox in Science](https://www.pnas.org/content/pnas/117/17/9284.full.pdf)
